{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ef3faf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (538774939.py, line 12)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mimport pandas as pdimport os\u001b[39m\n                              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pdimport os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507cbde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception Module Block (Feature Extraction Block)\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, f1, f2, f3, f4):\n",
    "        \"\"\"\n",
    "        Inception block that combines feature maps from different kernel sizes.\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Number of input channels\n",
    "            f1: Number of filters for 1x1 convolution\n",
    "            f2: Number of filters for 3x3 convolution\n",
    "            f3: Number of filters for 5x5 convolution\n",
    "            f4: Number of filters for max pooling path\n",
    "        \"\"\"\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        \n",
    "        # 1x1 convolution branch\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, f1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 3x3 convolution branch\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, f2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 5x5 convolution branch\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, f3, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(f3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Max pooling branch\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, f4, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(f4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        \n",
    "        # Concatenate all branches\n",
    "        outputs = torch.cat([branch1, branch2, branch3, branch4], dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83102fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asbestos Detection CNN with Inception-based Architecture\n",
    "class AsbestosDetectionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        \"\"\"\n",
    "        CNN architecture based on Inception-Net for asbestos roof detection.\n",
    "        Input: 100x100x3 RGB images\n",
    "        \"\"\"\n",
    "        super(AsbestosDetectionCNN, self).__init__()\n",
    "        \n",
    "        # Initial convolutional block\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        \n",
    "        # Four Inception feature extraction blocks\n",
    "        # Block 1: 64 -> 256 channels (64+64+64+64)\n",
    "        self.inception1 = InceptionBlock(64, 64, 64, 64, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout2d(p=0.55)\n",
    "        \n",
    "        # Block 2: 256 -> 512 channels (128+128+128+128)\n",
    "        self.inception2 = InceptionBlock(256, 128, 128, 128, 128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout2d(p=0.55)\n",
    "        \n",
    "        # Block 3: 512 -> 1024 channels (256+256+256+256)\n",
    "        self.inception3 = InceptionBlock(512, 256, 256, 256, 256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout3 = nn.Dropout2d(p=0.55)\n",
    "        \n",
    "        # Block 4: 1024 -> 1024 channels (256+256+256+256)\n",
    "        self.inception4 = InceptionBlock(1024, 256, 256, 256, 256)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout4 = nn.Dropout2d(p=0.55)\n",
    "        \n",
    "        # Calculate flattened size: 100x100 -> 50x50 -> 25x25 -> 12x12 -> 6x6\n",
    "        # After 4 pooling layers: 6x6x1024 = 36864\n",
    "        self.flatten_size = 6 * 6 * 1024\n",
    "        \n",
    "        # Two fully connected blocks\n",
    "        # FC Block 1\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 1024)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(1024)\n",
    "        self.relu_fc1 = nn.ReLU(inplace=True)\n",
    "        self.dropout_fc1 = nn.Dropout(p=0.50)\n",
    "        \n",
    "        # FC Block 2\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(1024)\n",
    "        self.relu_fc2 = nn.ReLU(inplace=True)\n",
    "        self.dropout_fc2 = nn.Dropout(p=0.50)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(1024, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial conv block\n",
    "        x = self.conv_block(x)\n",
    "        \n",
    "        # Inception blocks with pooling and dropout\n",
    "        x = self.inception1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.inception2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.inception3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.inception4(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC blocks\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc1(x)\n",
    "        x = self.relu_fc1(x)\n",
    "        x = self.dropout_fc1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn_fc2(x)\n",
    "        x = self.relu_fc2(x)\n",
    "        x = self.dropout_fc2(x)\n",
    "        \n",
    "        # Output with softmax\n",
    "        x = self.output(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64209616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for asbestos detection\n",
    "class AsbestosDataset(Dataset):\n",
    "    def __init__(self, images, labels, standardize=True):\n",
    "        \"\"\"\n",
    "        Dataset for asbestos roof images.\n",
    "        \n",
    "        Args:\n",
    "            images: numpy array of shape (N, 100, 100, 3)\n",
    "            labels: numpy array of shape (N,) with binary labels (0 or 1)\n",
    "            standardize: whether to apply feature-wise standardization\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.standardize = standardize\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].astype(np.float32)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Feature-wise standardization (per image)\n",
    "        if self.standardize:\n",
    "            mean = image.mean(axis=(0, 1), keepdims=True)\n",
    "            std = image.std(axis=(0, 1), keepdims=True) + 1e-7\n",
    "            image = (image - mean) / std\n",
    "        \n",
    "        # Convert to tensor and change from HWC to CHW format\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "class ReduceLROnPlateau:\n",
    "    def __init__(self, optimizer, patience=5, factor=0.001, min_lr=1e-6):\n",
    "        self.optimizer = optimizer\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.best_loss = float('inf')\n",
    "        self.wait = 0\n",
    "    \n",
    "    def step(self, val_loss):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self._reduce_lr()\n",
    "                self.wait = 0\n",
    "    \n",
    "    def _reduce_lr(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            old_lr = param_group['lr']\n",
    "            new_lr = max(old_lr - self.factor, self.min_lr)\n",
    "            param_group['lr'] = new_lr\n",
    "            if new_lr != old_lr:\n",
    "                print(f'Reducing learning rate from {old_lr:.6f} to {new_lr:.6f}')\n",
    "\n",
    "class StepLRScheduler:\n",
    "    def __init__(self, optimizer, step_size=10, gamma=0.85):\n",
    "        self.optimizer = optimizer\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        self.last_epoch = 0\n",
    "    \n",
    "    def step(self, epoch):\n",
    "        if (epoch + 1) % self.step_size == 0:\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                old_lr = param_group['lr']\n",
    "                new_lr = old_lr * self.gamma\n",
    "                param_group['lr'] = new_lr\n",
    "                print(f'Epoch {epoch+1}: Reducing learning rate from {old_lr:.6f} to {new_lr:.6f}')\n",
    "        self.last_epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac0265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=128, learning_rate=0.0015, device='cpu', save_dir='./models'):\n",
    "    \"\"\"\n",
    "    Train the asbestos detection model.\n",
    "    \n",
    "    Args:\n",
    "        model: AsbestosDetectionCNN model\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        num_epochs: Number of training epochs (default: 128)\n",
    "        learning_rate: Initial learning rate (default: 0.0015)\n",
    "        device: Device to train on ('cpu' or 'cuda')\n",
    "        save_dir: Directory to save model checkpoints\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing training history\n",
    "    \"\"\"\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Binary cross-entropy loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Adam optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Learning rate schedulers\n",
    "    step_scheduler = StepLRScheduler(optimizer, step_size=10, gamma=0.85)\n",
    "    plateau_scheduler = ReduceLROnPlateau(optimizer, patience=5, factor=0.001)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = None\n",
    "    \n",
    "    print(f\"Training on device: {device}\")\n",
    "    print(f\"Total epochs: {num_epochs}, Batch size: {train_loader.batch_size}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = 100. * train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100. * val_correct / val_total\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "        print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save best model based on validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_path = f\"{save_dir}/best_model_epoch_{epoch+1}.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': avg_val_loss,\n",
    "                'val_acc': val_accuracy\n",
    "            }, best_model_path)\n",
    "            print(f\"  ✓ Best model saved: {best_model_path}\")\n",
    "        \n",
    "        # Save checkpoint every epoch\n",
    "        checkpoint_path = f\"{save_dir}/checkpoint_epoch_{epoch+1}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': avg_val_loss,\n",
    "            'val_acc': val_accuracy\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Update learning rate schedulers\n",
    "        step_scheduler.step(epoch)\n",
    "        plateau_scheduler.step(avg_val_loss)\n",
    "    \n",
    "    print(f\"Training completed! Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best model saved at: {best_model_path}\")\n",
    "    \n",
    "    return history, best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0940346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_loader: DataLoader for test data\n",
    "        device: Device to evaluate on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probabilities = outputs.cpu().numpy()\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_probabilities.extend(probabilities)\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probabilities = np.array(all_probabilities)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='binary')\n",
    "    recall = recall_score(all_labels, all_predictions, average='binary')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': all_predictions,\n",
    "        'labels': all_labels,\n",
    "        'probabilities': all_probabilities\n",
    "    }\n",
    "    \n",
    "    print(\"Evaluation Results:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899b9087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training history including loss, accuracy, and learning rate.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "    axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "    axes[1].plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot learning rate\n",
    "    axes[2].plot(history['learning_rates'], linewidth=2, color='green')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Learning Rate')\n",
    "    axes[2].set_title('Learning Rate Schedule')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Training history plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names=['Non-Asbestos', 'Asbestos'], save_path=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix as a heatmap.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Confusion matrix saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def visualize_predictions(images, labels, predictions, probabilities, num_samples=16):\n",
    "    \"\"\"\n",
    "    Visualize sample predictions with their confidence scores.\n",
    "    \"\"\"\n",
    "    num_samples = min(num_samples, len(images))\n",
    "    indices = np.random.choice(len(images), num_samples, replace=False)\n",
    "    \n",
    "    rows = int(np.sqrt(num_samples))\n",
    "    cols = int(np.ceil(num_samples / rows))\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "    axes = axes.flatten() if num_samples > 1 else [axes]\n",
    "    \n",
    "    for idx, ax in enumerate(axes):\n",
    "        if idx >= num_samples:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        \n",
    "        i = indices[idx]\n",
    "        \n",
    "        # Get image (convert from CHW to HWC and denormalize)\n",
    "        img = images[i]\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Normalize to [0, 1] for display\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-7)\n",
    "        \n",
    "        true_label = labels[i]\n",
    "        pred_label = predictions[i]\n",
    "        confidence = probabilities[i][pred_label] * 100\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        \n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        title = f\"True: {'Asbestos' if true_label == 1 else 'Non-Asbestos'}\\n\"\n",
    "        title += f\"Pred: {'Asbestos' if pred_label == 1 else 'Non-Asbestos'} ({confidence:.1f}%)\"\n",
    "        \n",
    "        ax.set_title(title, color=color, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from folder and CSV\n",
    "def load_images_from_folder(csv_path='building_labels.csv', images_folder='building_images_labeled', image_size=(100, 100)):\n",
    "    \"\"\"\n",
    "    Wczytuje obrazy z folderu i etykiety z CSV\n",
    "    \n",
    "    Args:\n",
    "        csv_path: ścieżka do pliku CSV z etykietami\n",
    "        images_folder: folder z obrazami\n",
    "        image_size: docelowy rozmiar obrazów (100, 100)\n",
    "    \n",
    "    Returns:\n",
    "        images: numpy array (N, 100, 100, 3)\n",
    "        labels: numpy array (N,) z wartościami 0 lub 1\n",
    "        filenames: lista nazw plików\n",
    "    \"\"\"\n",
    "    # Wczytaj CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Wczytano {len(df)} wpisów z CSV\")\n",
    "    \n",
    "    images = []\n",
    "\n",
    "    labels = []    print(f\"  {label_name} ({label}): {count} ({count/len(labels)*100:.1f}%)\")\n",
    "\n",
    "    filenames = []    label_name = 'Azbest' if label == 1 else 'Bez azbestu'\n",
    "\n",
    "    for label, count in zip(unique, counts):\n",
    "\n",
    "    for idx, row in df.iterrows():print(f\"\\nRozkład klas:\")\n",
    "\n",
    "        filename = row['filename']unique, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "        has_asbestos = int(row['has_asbestos'])# Sprawdź rozkład klas\n",
    "\n",
    "        \n",
    "\n",
    "        img_path = os.path.join(images_folder, filename))\n",
    "\n",
    "            image_size=(100, 100)\n",
    "\n",
    "        if os.path.exists(img_path):    images_folder='building_images_labeled',\n",
    "\n",
    "            # Wczytaj i zmień rozmiar obrazu    csv_path='building_labels.csv',\n",
    "\n",
    "            img = Image.open(img_path).convert('RGB')images, labels, filenames = load_images_from_folder(\n",
    "\n",
    "            img = img.resize(image_size, Image.LANCZOS)print(\"Ładowanie danych...\")\n",
    "\n",
    "            img_array = np.array(img)# Załaduj wszystkie dane\n",
    "\n",
    "            \n",
    "\n",
    "            images.append(img_array)    return images, labels, filenames\n",
    "\n",
    "            labels.append(has_asbestos)    \n",
    "\n",
    "            filenames.append(filename)    print(f\"  Kształt etykiet: {labels.shape}\")\n",
    "\n",
    "        print(f\"  Kształt obrazów: {images.shape}\")\n",
    "\n",
    "    images = np.array(images, dtype=np.uint8)    print(f\"✓ Wczytano {len(images)} obrazów\")\n",
    "\n",
    "    labels = np.array(labels, dtype=np.int64)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podział na train/validation/test (80/10/10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\nPodział danych na train/validation/test...\")\n",
    "\n",
    "# Najpierw oddziel test set (10%)\n",
    "train_val_images, test_images, train_val_labels, test_labels = train_test_split(\n",
    "    images, labels, test_size=0.1, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Następnie podziel pozostałe na train i validation (90% / 10% z pozostałych = 81% / 9% całości)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_val_images, train_val_labels, test_size=0.111, random_state=42, stratify=train_val_labels\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_images)} ({len(train_images)/len(images)*100:.1f}%)\")\n",
    "print(f\"Validation: {len(val_images)} ({len(val_images)/len(images)*100:.1f}%)\")\n",
    "print(f\"Test: {len(test_images)} ({len(test_images)/len(images)*100:.1f}%)\")\n",
    "\n",
    "# Sprawdź rozkład klas w każdym zbiorze\n",
    "for name, lbls in [('Train', train_labels), ('Validation', val_labels), ('Test', test_labels)]:\n",
    "    unique, counts = np.unique(lbls, return_counts=True)\n",
    "    print(f\"\\n{name} - rozkład klas:\")\n",
    "    for label, count in zip(unique, counts):\n",
    "        print(f\"  Klasa {label}: {count} ({count/len(lbls)*100:.1f}%)\")\n",
    "\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Utwórz datasetyprint(f\"  Val batches: {len(val_loader)}\")\n",
    "\n",
    "train_dataset = AsbestosDataset(train_images, train_labels, standardize=True)print(f\"  Train batches: {len(train_loader)}\")\n",
    "\n",
    "val_dataset = AsbestosDataset(val_images, val_labels, standardize=True)print(f\"\\n✓ Loadery utworzone\")\n",
    "\n",
    "test_dataset = AsbestosDataset(test_images, test_labels, standardize=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "# Utwórz data loaderyval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56377717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and check architecture\n",
    "print(\"Initializing model...\")\n",
    "model = AsbestosDetectionCNN(num_classes=2)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Print model architecture summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(2, 3, 100, 100).to(device)\n",
    "model = model.to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"\\nTest forward pass - Input shape: {test_input.shape}, Output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "history, best_model_path = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=128,\n",
    "    learning_rate=0.0015,\n",
    "    device=device,\n",
    "    save_dir='./models'\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plot_training_history(history, save_path='./training_history.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4022b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "print(\"Loading best model for evaluation...\")\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"Validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "print(f\"Validation accuracy: {checkpoint['val_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afcb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "print(\"=\" * 70)\n",
    "test_results = evaluate_model(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15657ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(test_results['confusion_matrix'], \n",
    "                     class_names=['Non-Asbestos', 'Asbestos'],\n",
    "                     save_path='./confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on test samples\n",
    "# Get test data for visualization\n",
    "test_images_viz = test_images[:64]  # First 64 samples\n",
    "test_labels_viz = test_labels[:64]\n",
    "\n",
    "# Create a temporary dataset for visualization\n",
    "viz_dataset = AsbestosDataset(test_images_viz, test_labels_viz, standardize=True)\n",
    "viz_loader = DataLoader(viz_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Get predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in viz_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probabilities = outputs.cpu().numpy()\n",
    "        _, predictions = outputs.max(1)\n",
    "        predictions = predictions.cpu().numpy()\n",
    "        \n",
    "        visualize_predictions(images.cpu(), labels.numpy(), predictions, probabilities, num_samples=16)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0680c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model for deployment\n",
    "final_model_path = './models/asbestos_detection_final.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_architecture': 'AsbestosDetectionCNN',\n",
    "    'input_size': (100, 100, 3),\n",
    "    'num_classes': 2,\n",
    "    'test_accuracy': test_results['accuracy'],\n",
    "    'test_f1_score': test_results['f1_score']\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"Final model saved to: {final_model_path}\")\n",
    "print(f\"Test Accuracy: {test_results['accuracy']:.4f}\")\n",
    "print(f\"Test F1-Score: {test_results['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b9450",
   "metadata": {},
   "source": [
    "# Asbestos Roof Detection with CNN\n",
    "\n",
    "## Model Architecture\n",
    "This implementation is based on the paper's CNN architecture with inception-net feature extraction blocks. The model uses:\n",
    "\n",
    "- **Input**: 100x100x3 RGB images\n",
    "- **Initial Conv Block**: 64 filters, 3x3 kernel\n",
    "- **4 Inception Blocks**: Each combining 1x1, 3x3, 5x5 convolutions and max pooling\n",
    "- **Pooling & Dropout**: After each inception block (dropout rate 0.55)\n",
    "- **2 Fully Connected Blocks**: 1024 neurons each with batch normalization and dropout (0.50)\n",
    "- **Output**: Softmax layer for binary classification\n",
    "\n",
    "## Training Configuration\n",
    "- **Epochs**: 128\n",
    "- **Batch Size**: 64\n",
    "- **Initial Learning Rate**: 0.0015\n",
    "- **Optimizer**: Adam\n",
    "- **Loss Function**: Binary Cross-Entropy\n",
    "- **LR Schedule**: Reduced by 15% every 10 epochs + plateau-based reduction\n",
    "- **Standardization**: Feature-wise standardization applied to each image\n",
    "\n",
    "## Key Features\n",
    "1. Inception blocks enable multi-scale feature extraction\n",
    "2. Feature-wise standardization handles variable lighting conditions\n",
    "3. Aggressive dropout (0.55 spatial, 0.50 regular) prevents overfitting\n",
    "4. Model checkpointing saves best model based on validation loss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5ef3faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "507cbde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception Module Block (Feature Extraction Block)\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, f1, f2, f3, f4):\n",
    "        \"\"\"\n",
    "        Inception block that combines feature maps from different kernel sizes.\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Number of input channels\n",
    "            f1: Number of filters for 1x1 convolution\n",
    "            f2: Number of filters for 3x3 convolution\n",
    "            f3: Number of filters for 5x5 convolution\n",
    "            f4: Number of filters for max pooling path\n",
    "        \"\"\"\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        \n",
    "        # 1x1 convolution branch\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, f1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 3x3 convolution branch\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, f2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 5x5 convolution branch\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, f3, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(f3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Max pooling branch\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, f4, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(f4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        branch1 = self.branch1(x)\n",
    "        branch2 = self.branch2(x)\n",
    "        branch3 = self.branch3(x)\n",
    "        branch4 = self.branch4(x)\n",
    "        \n",
    "        # Concatenate all branches\n",
    "        outputs = torch.cat([branch1, branch2, branch3, branch4], dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "83102fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asbestos Detection CNN with Inception-based Architecture\n",
    "class AsbestosDetectionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        \"\"\"\n",
    "        CNN architecture based on Inception-Net for asbestos roof detection.\n",
    "        Input: 100x100x3 RGB images\n",
    "        \"\"\"\n",
    "        super(AsbestosDetectionCNN, self).__init__()\n",
    "        \n",
    "        # Initial convolutional block\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        \n",
    "        # Four Inception feature extraction blocks\n",
    "        # Block 1: 64 -> 256 channels (64+64+64+64)\n",
    "        self.inception1 = InceptionBlock(64, 64, 64, 64, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout2d(p=0.55)\n",
    "        \n",
    "        # Block 2: 256 -> 512 channels (128+128+128+128)\n",
    "        self.inception2 = InceptionBlock(256, 128, 128, 128, 128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout2d(p=0.55)\n",
    "        \n",
    "        # Block 3: 512 -> 1024 channels (256+256+256+256)\n",
    "        self.inception3 = InceptionBlock(512, 256, 256, 256, 256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout3 = nn.Dropout2d(p=0.55)\n",
    "        \n",
    "        # Block 4: 1024 -> 1024 channels (256+256+256+256)\n",
    "        self.inception4 = InceptionBlock(1024, 256, 256, 256, 256)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout4 = nn.Dropout2d(p=0.55)\n",
    "        \n",
    "        # Calculate flattened size: 100x100 -> 50x50 -> 25x25 -> 12x12 -> 6x6\n",
    "        # After 4 pooling layers: 6x6x1024 = 36864\n",
    "        self.flatten_size = 6 * 6 * 1024\n",
    "        \n",
    "        # Two fully connected blocks\n",
    "        # FC Block 1\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 1024)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(1024)\n",
    "        self.relu_fc1 = nn.ReLU(inplace=True)\n",
    "        self.dropout_fc1 = nn.Dropout(p=0.50)\n",
    "        \n",
    "        # FC Block 2\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(1024)\n",
    "        self.relu_fc2 = nn.ReLU(inplace=True)\n",
    "        self.dropout_fc2 = nn.Dropout(p=0.50)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(1024, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial conv block\n",
    "        x = self.conv_block(x)\n",
    "        \n",
    "        # Inception blocks with pooling and dropout\n",
    "        x = self.inception1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.inception2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.inception3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.inception4(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC blocks\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn_fc1(x)\n",
    "        x = self.relu_fc1(x)\n",
    "        x = self.dropout_fc1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn_fc2(x)\n",
    "        x = self.relu_fc2(x)\n",
    "        x = self.dropout_fc2(x)\n",
    "        \n",
    "        # Output with softmax\n",
    "        x = self.output(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "64209616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for asbestos detection\n",
    "class AsbestosDataset(Dataset):\n",
    "    def __init__(self, images, labels, standardize=True):\n",
    "        \"\"\"\n",
    "        Dataset for asbestos roof images.\n",
    "        \n",
    "        Args:\n",
    "            images: numpy array of shape (N, 100, 100, 3)\n",
    "            labels: numpy array of shape (N,) with binary labels (0 or 1)\n",
    "            standardize: whether to apply feature-wise standardization\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.standardize = standardize\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].astype(np.float32)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Feature-wise standardization (per image)\n",
    "        if self.standardize:\n",
    "            mean = image.mean(axis=(0, 1), keepdims=True)\n",
    "            std = image.std(axis=(0, 1), keepdims=True) + 1e-7\n",
    "            image = (image - mean) / std\n",
    "        \n",
    "        # Convert to tensor and change from HWC to CHW format\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fad5711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "class ReduceLROnPlateau:\n",
    "    def __init__(self, optimizer, patience=5, factor=0.001, min_lr=1e-6):\n",
    "        self.optimizer = optimizer\n",
    "        self.patience = patience\n",
    "        self.factor = factor\n",
    "        self.min_lr = min_lr\n",
    "        self.best_loss = float('inf')\n",
    "        self.wait = 0\n",
    "    \n",
    "    def step(self, val_loss):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self._reduce_lr()\n",
    "                self.wait = 0\n",
    "    \n",
    "    def _reduce_lr(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            old_lr = param_group['lr']\n",
    "            new_lr = max(old_lr - self.factor, self.min_lr)\n",
    "            param_group['lr'] = new_lr\n",
    "            if new_lr != old_lr:\n",
    "                print(f'Reducing learning rate from {old_lr:.6f} to {new_lr:.6f}')\n",
    "\n",
    "class StepLRScheduler:\n",
    "    def __init__(self, optimizer, step_size=10, gamma=0.85):\n",
    "        self.optimizer = optimizer\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        self.last_epoch = 0\n",
    "    \n",
    "    def step(self, epoch):\n",
    "        if (epoch + 1) % self.step_size == 0:\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                old_lr = param_group['lr']\n",
    "                new_lr = old_lr * self.gamma\n",
    "                param_group['lr'] = new_lr\n",
    "                print(f'Epoch {epoch+1}: Reducing learning rate from {old_lr:.6f} to {new_lr:.6f}')\n",
    "        self.last_epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e8ac0265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=128, learning_rate=0.0015, device='cpu', save_dir='./models'):\n",
    "    \"\"\"\n",
    "    Train the asbestos detection model.\n",
    "    \n",
    "    Args:\n",
    "        model: AsbestosDetectionCNN model\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        num_epochs: Number of training epochs (default: 128)\n",
    "        learning_rate: Initial learning rate (default: 0.0015)\n",
    "        device: Device to train on ('cpu' or 'cuda')\n",
    "        save_dir: Directory to save model checkpoints\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing training history\n",
    "    \"\"\"\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Binary cross-entropy loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Adam optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Learning rate schedulers\n",
    "    step_scheduler = StepLRScheduler(optimizer, step_size=10, gamma=0.85)\n",
    "    plateau_scheduler = ReduceLROnPlateau(optimizer, patience=5, factor=0.001)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = None\n",
    "    \n",
    "    print(f\"Training on device: {device}\")\n",
    "    print(f\"Total epochs: {num_epochs}, Batch size: {train_loader.batch_size}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = 100. * train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100. * val_correct / val_total\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}%\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "        print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save best model based on validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_path = f\"{save_dir}/best_model_epoch_{epoch+1}.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': avg_val_loss,\n",
    "                'val_acc': val_accuracy\n",
    "            }, best_model_path)\n",
    "            print(f\"  ✓ Best model saved: {best_model_path}\")\n",
    "        \n",
    "        # Save checkpoint every epoch\n",
    "        checkpoint_path = f\"{save_dir}/checkpoint_epoch_{epoch+1}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': avg_val_loss,\n",
    "            'val_acc': val_accuracy\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Update learning rate schedulers\n",
    "        step_scheduler.step(epoch)\n",
    "        plateau_scheduler.step(avg_val_loss)\n",
    "    \n",
    "    print(f\"Training completed! Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best model saved at: {best_model_path}\")\n",
    "    \n",
    "    return history, best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f0940346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_loader: DataLoader for test data\n",
    "        device: Device to evaluate on\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probabilities = outputs.cpu().numpy()\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_probabilities.extend(probabilities)\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probabilities = np.array(all_probabilities)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='binary')\n",
    "    recall = recall_score(all_labels, all_predictions, average='binary')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': all_predictions,\n",
    "        'labels': all_labels,\n",
    "        'probabilities': all_probabilities\n",
    "    }\n",
    "    \n",
    "    print(\"Evaluation Results:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "899b9087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training history including loss, accuracy, and learning rate.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "    axes[0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[1].plot(history['train_acc'], label='Train Accuracy', linewidth=2)\n",
    "    axes[1].plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot learning rate\n",
    "    axes[2].plot(history['learning_rates'], linewidth=2, color='green')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Learning Rate')\n",
    "    axes[2].set_title('Learning Rate Schedule')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Training history plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names=['Non-Asbestos', 'Asbestos'], save_path=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix as a heatmap.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Confusion matrix saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def visualize_predictions(images, labels, predictions, probabilities, num_samples=16):\n",
    "    \"\"\"\n",
    "    Visualize sample predictions with their confidence scores.\n",
    "    \"\"\"\n",
    "    num_samples = min(num_samples, len(images))\n",
    "    indices = np.random.choice(len(images), num_samples, replace=False)\n",
    "    \n",
    "    rows = int(np.sqrt(num_samples))\n",
    "    cols = int(np.ceil(num_samples / rows))\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "    axes = axes.flatten() if num_samples > 1 else [axes]\n",
    "    \n",
    "    for idx, ax in enumerate(axes):\n",
    "        if idx >= num_samples:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        \n",
    "        i = indices[idx]\n",
    "        \n",
    "        # Get image (convert from CHW to HWC and denormalize)\n",
    "        img = images[i]\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Normalize to [0, 1] for display\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-7)\n",
    "        \n",
    "        true_label = labels[i]\n",
    "        pred_label = predictions[i]\n",
    "        confidence = probabilities[i][pred_label] * 100\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        \n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        title = f\"True: {'Asbestos' if true_label == 1 else 'Non-Asbestos'}\\n\"\n",
    "        title += f\"Pred: {'Asbestos' if pred_label == 1 else 'Non-Asbestos'} ({confidence:.1f}%)\"\n",
    "        \n",
    "        ax.set_title(title, color=color, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9fbd869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ładowanie danych...\n",
      "Wczytano 282 wpisów z CSV\n",
      "✓ Wczytano 282 obrazów\n",
      "  Kształt obrazów: (282, 100, 100, 3)\n",
      "  Kształt etykiet: (282,)\n",
      "\n",
      "Rozkład klas:\n",
      "  Bez azbestu (0): 265 (94.0%)\n",
      "  Azbest (1): 17 (6.0%)\n"
     ]
    }
   ],
   "source": [
    "# Load data from folder and CSV\n",
    "def load_images_from_folder(csv_path='building_labels.csv', images_folder='building_images_labeled', image_size=(100, 100)):\n",
    "    \"\"\"\n",
    "    Wczytuje obrazy z folderu i etykiety z CSV\n",
    "    \n",
    "    Args:\n",
    "        csv_path: ścieżka do pliku CSV z etykietami\n",
    "        images_folder: folder z obrazami\n",
    "        image_size: docelowy rozmiar obrazów (100, 100)\n",
    "    \n",
    "    Returns:\n",
    "        images: numpy array (N, 100, 100, 3)\n",
    "        labels: numpy array (N,) z wartościami 0 lub 1\n",
    "        filenames: lista nazw plików\n",
    "    \"\"\"\n",
    "    # Wczytaj CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Wczytano {len(df)} wpisów z CSV\")\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    filenames = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        filename = row['filename']\n",
    "        has_asbestos = int(row['has_asbestos'])\n",
    "        \n",
    "        img_path = os.path.join(images_folder, filename)\n",
    "        \n",
    "        if os.path.exists(img_path):\n",
    "            # Wczytaj i zmień rozmiar obrazu\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = img.resize(image_size, Image.LANCZOS)\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            images.append(img_array)\n",
    "            labels.append(has_asbestos)\n",
    "            filenames.append(filename)\n",
    "    \n",
    "    images = np.array(images, dtype=np.uint8)\n",
    "    labels = np.array(labels, dtype=np.int64)\n",
    "    \n",
    "    print(f\"✓ Wczytano {len(images)} obrazów\")\n",
    "    print(f\"  Kształt obrazów: {images.shape}\")\n",
    "    print(f\"  Kształt etykiet: {labels.shape}\")\n",
    "    \n",
    "    # Sprawdź rozkład klas\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"\\nRozkład klas:\")\n",
    "    for label, count in zip(unique, counts):\n",
    "        label_name = 'Azbest' if label == 1 else 'Bez azbestu'\n",
    "        print(f\"  {label_name} ({label}): {count} ({count/len(labels)*100:.1f}%)\")\n",
    "    \n",
    "    return images, labels, filenames\n",
    "\n",
    "# Załaduj wszystkie dane\n",
    "print(\"Ładowanie danych...\")\n",
    "images, labels, filenames = load_images_from_folder(\n",
    "    csv_path='building_labels.csv',\n",
    "    images_folder='building_images_labeled',\n",
    "    image_size=(100, 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "08db0a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Podział danych na train/validation/test...\n",
      "Train: 224 (79.4%)\n",
      "Validation: 29 (10.3%)\n",
      "Test: 29 (10.3%)\n",
      "\n",
      "Train - rozkład klas:\n",
      "  Klasa 0: 211 (94.2%)\n",
      "  Klasa 1: 13 (5.8%)\n",
      "\n",
      "Validation - rozkład klas:\n",
      "  Klasa 0: 27 (93.1%)\n",
      "  Klasa 1: 2 (6.9%)\n",
      "\n",
      "Test - rozkład klas:\n",
      "  Klasa 0: 27 (93.1%)\n",
      "  Klasa 1: 2 (6.9%)\n",
      "\n",
      "✓ Loadery utworzone\n",
      "  Train batches: 4\n",
      "  Val batches: 1\n",
      "  Test batches: 1\n"
     ]
    }
   ],
   "source": [
    "# Podział na train/validation/test (80/10/10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\nPodział danych na train/validation/test...\")\n",
    "\n",
    "# Najpierw oddziel test set (10%)\n",
    "train_val_images, test_images, train_val_labels, test_labels = train_test_split(\n",
    "    images, labels, test_size=0.1, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Następnie podziel pozostałe na train i validation (90% / 10% z pozostałych = 81% / 9% całości)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    train_val_images, train_val_labels, test_size=0.111, random_state=42, stratify=train_val_labels\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_images)} ({len(train_images)/len(images)*100:.1f}%)\")\n",
    "print(f\"Validation: {len(val_images)} ({len(val_images)/len(images)*100:.1f}%)\")\n",
    "print(f\"Test: {len(test_images)} ({len(test_images)/len(images)*100:.1f}%)\")\n",
    "\n",
    "# Sprawdź rozkład klas w każdym zbiorze\n",
    "for name, lbls in [('Train', train_labels), ('Validation', val_labels), ('Test', test_labels)]:\n",
    "    unique, counts = np.unique(lbls, return_counts=True)\n",
    "    print(f\"\\n{name} - rozkład klas:\")\n",
    "    for label, count in zip(unique, counts):\n",
    "        print(f\"  Klasa {label}: {count} ({count/len(lbls)*100:.1f}%)\")\n",
    "\n",
    "# Utwórz datasety\n",
    "train_dataset = AsbestosDataset(train_images, train_labels, standardize=True)\n",
    "val_dataset = AsbestosDataset(val_images, val_labels, standardize=True)\n",
    "test_dataset = AsbestosDataset(test_images, test_labels, standardize=True)\n",
    "\n",
    "# Utwórz data loadery\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\n✓ Loadery utworzone\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "56377717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Total parameters: 54,298,754\n",
      "Trainable parameters: 54,298,754\n",
      "\n",
      "Model Architecture:\n",
      "AsbestosDetectionCNN(\n",
      "  (conv_block): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (inception1): InceptionBlock(\n",
      "    (branch1): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (branch2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout1): Dropout2d(p=0.55, inplace=False)\n",
      "  (inception2): InceptionBlock(\n",
      "    (branch1): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (branch2): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout2): Dropout2d(p=0.55, inplace=False)\n",
      "  (inception3): InceptionBlock(\n",
      "    (branch1): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (branch2): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): Conv2d(512, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout3): Dropout2d(p=0.55, inplace=False)\n",
      "  (inception4): InceptionBlock(\n",
      "    (branch1): Sequential(\n",
      "      (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (branch2): Sequential(\n",
      "      (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (branch3): Sequential(\n",
      "      (0): Conv2d(1024, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (branch4): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout4): Dropout2d(p=0.55, inplace=False)\n",
      "  (fc1): Linear(in_features=36864, out_features=1024, bias=True)\n",
      "  (bn_fc1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_fc1): ReLU(inplace=True)\n",
      "  (dropout_fc1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (bn_fc2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu_fc2): ReLU(inplace=True)\n",
      "  (dropout_fc2): Dropout(p=0.5, inplace=False)\n",
      "  (output): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "Using device: mps\n",
      "\n",
      "Test forward pass - Input shape: torch.Size([2, 3, 100, 100]), Output shape: torch.Size([2, 2])\n",
      "\n",
      "Test forward pass - Input shape: torch.Size([2, 3, 100, 100]), Output shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and check architecture\n",
    "print(\"Initializing model...\")\n",
    "model = AsbestosDetectionCNN(num_classes=2)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Print model architecture summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Check if MPS is available\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(2, 3, 100, 100).to(device)\n",
    "model = model.to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"\\nTest forward pass - Input shape: {test_input.shape}, Output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e08b3123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "======================================================================\n",
      "Training on device: mps\n",
      "Total epochs: 128, Batch size: 64\n",
      "======================================================================\n",
      "Epoch [1/128]\n",
      "  Train Loss: 0.5419, Train Acc: 74.55%\n",
      "  Val Loss: 0.3825, Val Acc: 93.10%\n",
      "  Learning Rate: 0.001500\n",
      "Epoch [1/128]\n",
      "  Train Loss: 0.5419, Train Acc: 74.55%\n",
      "  Val Loss: 0.3825, Val Acc: 93.10%\n",
      "  Learning Rate: 0.001500\n",
      "  ✓ Best model saved: ./models/best_model_epoch_1.pth\n",
      "  ✓ Best model saved: ./models/best_model_epoch_1.pth\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [2/128]\n",
      "  Train Loss: 0.3745, Train Acc: 94.20%\n",
      "  Val Loss: 0.3890, Val Acc: 93.10%\n",
      "  Learning Rate: 0.001500\n",
      "Epoch [2/128]\n",
      "  Train Loss: 0.3745, Train Acc: 94.20%\n",
      "  Val Loss: 0.3890, Val Acc: 93.10%\n",
      "  Learning Rate: 0.001500\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [3/128]\n",
      "  Train Loss: 0.3725, Train Acc: 94.20%\n",
      "  Val Loss: 0.4112, Val Acc: 93.10%\n",
      "  Learning Rate: 0.001500\n",
      "Epoch [3/128]\n",
      "  Train Loss: 0.3725, Train Acc: 94.20%\n",
      "  Val Loss: 0.4112, Val Acc: 93.10%\n",
      "  Learning Rate: 0.001500\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [4/128]\n",
      "  Train Loss: 0.3720, Train Acc: 94.20%\n",
      "  Val Loss: 0.3899, Val Acc: 93.10%\n",
      "  Learning Rate: 0.001500\n",
      "Epoch [4/128]\n",
      "  Train Loss: 0.3720, Train Acc: 94.20%\n",
      "  Val Loss: 0.3899, Val Acc: 93.10%\n",
      "  Learning Rate: 0.001500\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [5/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3853, Val Acc: 93.10%\n",
      "  Learning Rate: 0.001500\n",
      "Epoch [5/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3853, Val Acc: 93.10%\n",
      "  Learning Rate: 0.001500\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [6/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3831, Val Acc: 93.10%\n",
      "  Learning Rate: 0.001500\n",
      "Epoch [6/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3831, Val Acc: 93.10%\n",
      "  Learning Rate: 0.001500\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.001500 to 0.000500\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.001500 to 0.000500\n",
      "Epoch [7/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3824, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000500\n",
      "Epoch [7/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3824, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000500\n",
      "  ✓ Best model saved: ./models/best_model_epoch_7.pth\n",
      "  ✓ Best model saved: ./models/best_model_epoch_7.pth\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [8/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3823, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000500\n",
      "Epoch [8/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3823, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000500\n",
      "  ✓ Best model saved: ./models/best_model_epoch_8.pth\n",
      "  ✓ Best model saved: ./models/best_model_epoch_8.pth\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [9/128]\n",
      "  Train Loss: 0.3836, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000500\n",
      "Epoch [9/128]\n",
      "  Train Loss: 0.3836, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000500\n",
      "  ✓ Best model saved: ./models/best_model_epoch_9.pth\n",
      "  ✓ Best model saved: ./models/best_model_epoch_9.pth\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [10/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000500\n",
      "Epoch [10/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000500\n",
      "  ✓ Best model saved: ./models/best_model_epoch_10.pth\n",
      "  ✓ Best model saved: ./models/best_model_epoch_10.pth\n",
      "----------------------------------------------------------------------\n",
      "Epoch 10: Reducing learning rate from 0.000500 to 0.000425\n",
      "----------------------------------------------------------------------\n",
      "Epoch 10: Reducing learning rate from 0.000500 to 0.000425\n",
      "Epoch [11/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000425\n",
      "Epoch [11/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000425\n",
      "  ✓ Best model saved: ./models/best_model_epoch_11.pth\n",
      "  ✓ Best model saved: ./models/best_model_epoch_11.pth\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [12/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000425\n",
      "Epoch [12/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000425\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [13/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000425\n",
      "Epoch [13/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000425\n",
      "  ✓ Best model saved: ./models/best_model_epoch_13.pth\n",
      "  ✓ Best model saved: ./models/best_model_epoch_13.pth\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [14/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000425\n",
      "Epoch [14/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000425\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [15/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000425\n",
      "Epoch [15/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000425\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [16/128]\n",
      "  Train Loss: 0.3797, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000425\n",
      "Epoch [16/128]\n",
      "  Train Loss: 0.3797, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000425\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [17/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000425\n",
      "Epoch [17/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000425\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [18/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000425\n",
      "Epoch [18/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000425\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000425 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000425 to 0.000001\n",
      "Epoch [19/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [19/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [20/128]\n",
      "  Train Loss: 0.3836, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [20/128]\n",
      "  Train Loss: 0.3836, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Epoch 20: Reducing learning rate from 0.000001 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Epoch 20: Reducing learning rate from 0.000001 to 0.000001\n",
      "Epoch [21/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [21/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [22/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [22/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [23/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [23/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000001 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000001 to 0.000001\n",
      "Epoch [24/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [24/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [25/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [25/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [26/128]\n",
      "  Train Loss: 0.3640, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [26/128]\n",
      "  Train Loss: 0.3640, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [27/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [27/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [28/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [28/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [29/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [29/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [30/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [30/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Epoch 30: Reducing learning rate from 0.000001 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Epoch 30: Reducing learning rate from 0.000001 to 0.000001\n",
      "Epoch [31/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [31/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [32/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [32/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [33/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [33/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000001 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000001 to 0.000001\n",
      "Epoch [34/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [34/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [35/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [35/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [36/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [36/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [37/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [37/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [38/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [38/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [39/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [39/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [40/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [40/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Epoch 40: Reducing learning rate from 0.000001 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Epoch 40: Reducing learning rate from 0.000001 to 0.000001\n",
      "Epoch [41/128]\n",
      "  Train Loss: 0.3836, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [41/128]\n",
      "  Train Loss: 0.3836, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [42/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [42/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [43/128]\n",
      "  Train Loss: 0.3797, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [43/128]\n",
      "  Train Loss: 0.3797, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000001 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000001 to 0.000001\n",
      "Epoch [44/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [44/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [45/128]\n",
      "  Train Loss: 0.3836, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [45/128]\n",
      "  Train Loss: 0.3836, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [46/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [46/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [47/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [47/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [48/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [48/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [49/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [49/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [50/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [50/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Epoch 50: Reducing learning rate from 0.000001 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Epoch 50: Reducing learning rate from 0.000001 to 0.000001\n",
      "Epoch [51/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [51/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [52/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [52/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [53/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [53/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000001 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000001 to 0.000001\n",
      "Epoch [54/128]\n",
      "  Train Loss: 0.3679, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [54/128]\n",
      "  Train Loss: 0.3679, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [55/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [55/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [56/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [56/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [57/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [57/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [58/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [58/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [59/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [59/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [60/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [60/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Epoch 60: Reducing learning rate from 0.000001 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Epoch 60: Reducing learning rate from 0.000001 to 0.000001\n",
      "Epoch [61/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [61/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [62/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [62/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [63/128]\n",
      "  Train Loss: 0.3718, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [63/128]\n",
      "  Train Loss: 0.3718, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000001 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000001 to 0.000001\n",
      "Epoch [64/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [64/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [65/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [65/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [66/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [66/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [67/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [67/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [68/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [68/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [69/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [69/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [70/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [70/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Epoch 70: Reducing learning rate from 0.000001 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Epoch 70: Reducing learning rate from 0.000001 to 0.000001\n",
      "Epoch [71/128]\n",
      "  Train Loss: 0.3679, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [71/128]\n",
      "  Train Loss: 0.3679, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [72/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [72/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [73/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [73/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000001 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000001 to 0.000001\n",
      "Epoch [74/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [74/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [75/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [75/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [76/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [76/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [77/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [77/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [78/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [78/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [79/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [79/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [80/128]\n",
      "  Train Loss: 0.3718, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [80/128]\n",
      "  Train Loss: 0.3718, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Epoch 80: Reducing learning rate from 0.000001 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Epoch 80: Reducing learning rate from 0.000001 to 0.000001\n",
      "Epoch [81/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [81/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [82/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [82/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [83/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [83/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000001 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000001 to 0.000001\n",
      "Epoch [84/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [84/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [85/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [85/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [86/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [86/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [87/128]\n",
      "  Train Loss: 0.3797, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [87/128]\n",
      "  Train Loss: 0.3797, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [88/128]\n",
      "  Train Loss: 0.3797, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [88/128]\n",
      "  Train Loss: 0.3797, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [89/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [89/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [90/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [90/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Epoch 90: Reducing learning rate from 0.000001 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Epoch 90: Reducing learning rate from 0.000001 to 0.000001\n",
      "Epoch [91/128]\n",
      "  Train Loss: 0.3797, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [91/128]\n",
      "  Train Loss: 0.3797, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [92/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [92/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [93/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [93/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000001 to 0.000001\n",
      "----------------------------------------------------------------------\n",
      "Reducing learning rate from 0.000001 to 0.000001\n",
      "Epoch [94/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [94/128]\n",
      "  Train Loss: 0.3758, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [95/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [95/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [96/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [96/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [97/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [97/128]\n",
      "  Train Loss: 0.3680, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [98/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [98/128]\n",
      "  Train Loss: 0.3719, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Epoch [99/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "Epoch [99/128]\n",
      "  Train Loss: 0.3641, Train Acc: 94.20%\n",
      "  Val Loss: 0.3822, Val Acc: 93.10%\n",
      "  Learning Rate: 0.000001\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m history, best_model_path = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0015\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./models\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining completed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, num_epochs, learning_rate, device, save_dir)\u001b[39m\n\u001b[32m     60\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     61\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m train_loss += loss.item()\n\u001b[32m     65\u001b[39m _, predicted = outputs.max(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MPSI_LAB_PROJEKT/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:526\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    521\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    522\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    523\u001b[39m             )\n\u001b[32m    525\u001b[39m \u001b[38;5;66;03m# pyrefly: ignore [invalid-param-spec]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MPSI_LAB_PROJEKT/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:83\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     81\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     82\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     85\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MPSI_LAB_PROJEKT/.venv/lib/python3.12/site-packages/torch/optim/adam.py:248\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    236\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    238\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    239\u001b[39m         group,\n\u001b[32m    240\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m         state_steps,\n\u001b[32m    246\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MPSI_LAB_PROJEKT/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:153\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MPSI_LAB_PROJEKT/.venv/lib/python3.12/site-packages/torch/optim/adam.py:970\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    967\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    968\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MPSI_LAB_PROJEKT/.venv/lib/python3.12/site-packages/torch/optim/adam.py:476\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    472\u001b[39m         exp_avg_sq.mul_(beta2).addcmul_(\n\u001b[32m    473\u001b[39m             grad, grad, value=cast(\u001b[38;5;28mfloat\u001b[39m, \u001b[32m1\u001b[39m - beta2)\n\u001b[32m    474\u001b[39m         )\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    479\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "history, best_model_path = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=128,\n",
    "    learning_rate=0.0015,\n",
    "    device=device,\n",
    "    save_dir='./models'\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3202567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plot_training_history(history, save_path='./training_history.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4022b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "print(\"Loading best model for evaluation...\")\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"Validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "print(f\"Validation accuracy: {checkpoint['val_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afcb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "print(\"=\" * 70)\n",
    "test_results = evaluate_model(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15657ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(test_results['confusion_matrix'], \n",
    "                     class_names=['Non-Asbestos', 'Asbestos'],\n",
    "                     save_path='./confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on test samples\n",
    "# Get test data for visualization\n",
    "test_images_viz = test_images[:64]  # First 64 samples\n",
    "test_labels_viz = test_labels[:64]\n",
    "\n",
    "# Create a temporary dataset for visualization\n",
    "viz_dataset = AsbestosDataset(test_images_viz, test_labels_viz, standardize=True)\n",
    "viz_loader = DataLoader(viz_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Get predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in viz_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probabilities = outputs.cpu().numpy()\n",
    "        _, predictions = outputs.max(1)\n",
    "        predictions = predictions.cpu().numpy()\n",
    "        \n",
    "        visualize_predictions(images.cpu(), labels.numpy(), predictions, probabilities, num_samples=16)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0680c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model for deployment\n",
    "final_model_path = './models/asbestos_detection_final.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_architecture': 'AsbestosDetectionCNN',\n",
    "    'input_size': (100, 100, 3),\n",
    "    'num_classes': 2,\n",
    "    'test_accuracy': test_results['accuracy'],\n",
    "    'test_f1_score': test_results['f1_score']\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"Final model saved to: {final_model_path}\")\n",
    "print(f\"Test Accuracy: {test_results['accuracy']:.4f}\")\n",
    "print(f\"Test F1-Score: {test_results['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b9450",
   "metadata": {},
   "source": [
    "# Asbestos Roof Detection with CNN\n",
    "\n",
    "## Model Architecture\n",
    "This implementation is based on the paper's CNN architecture with inception-net feature extraction blocks. The model uses:\n",
    "\n",
    "- **Input**: 100x100x3 RGB images\n",
    "- **Initial Conv Block**: 64 filters, 3x3 kernel\n",
    "- **4 Inception Blocks**: Each combining 1x1, 3x3, 5x5 convolutions and max pooling\n",
    "- **Pooling & Dropout**: After each inception block (dropout rate 0.55)\n",
    "- **2 Fully Connected Blocks**: 1024 neurons each with batch normalization and dropout (0.50)\n",
    "- **Output**: Softmax layer for binary classification\n",
    "\n",
    "## Training Configuration\n",
    "- **Epochs**: 128\n",
    "- **Batch Size**: 64\n",
    "- **Initial Learning Rate**: 0.0015\n",
    "- **Optimizer**: Adam\n",
    "- **Loss Function**: Binary Cross-Entropy\n",
    "- **LR Schedule**: Reduced by 15% every 10 epochs + plateau-based reduction\n",
    "- **Standardization**: Feature-wise standardization applied to each image\n",
    "\n",
    "## Key Features\n",
    "1. Inception blocks enable multi-scale feature extraction\n",
    "2. Feature-wise standardization handles variable lighting conditions\n",
    "3. Aggressive dropout (0.55 spatial, 0.50 regular) prevents overfitting\n",
    "4. Model checkpointing saves best model based on validation loss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
